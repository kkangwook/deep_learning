{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90347412-572f-4973-96b6-125853f2374a",
   "metadata": {},
   "source": [
    "# 사이트\n",
    "- ncbi virus\n",
    "- virushostdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ca8970-4435-44f1-8dd0-e0135b83735b",
   "metadata": {},
   "source": [
    "# k-mer\n",
    "- 단일염기 A, C, G, T는 정보량이 너무 적음\n",
    "- k-mer는 DNA에서 의미 단위의 토큰 역할을 함 -> 단어처럼 작동\n",
    "- 3-mer: 문맥 포함, 의미 있는 단위\n",
    "- 6-mer: 더 긴 문맥 가능, 희소성 증가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf002b2-4a40-4096-a9fa-76898508d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5363e25f-563e-4889-9af0-9445e23fa89d",
   "metadata": {},
   "source": [
    "# 기본 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5750691b-55c1-491d-8d9f-2746d4db8786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "x # 유전자서열 컬럼\n",
    "y # 0 or 1의 정답\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=123)\n",
    "x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=0.2,random_state=123)\n",
    "\n",
    "model_name = \"zhihan1996/DNABERT-6\"\n",
    "# model_name = \"nucleotide-transformer/dna-bert-500k\" 이것도 해보기\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "def kmer_tokenizer(seq, k=6):\n",
    "    return ' '.join([seq[i:i+k] for i in range(len(seq) - k + 1)])\n",
    "    \n",
    "k=6\n",
    "x_train=[kmer_tokenizer(seq, k) for seq in x_train]\n",
    "x_val=[kmer_tokenizer(seq, k) for seq in x_val]\n",
    "x_test=[kmer_tokenizer(seq, k) for seq in x_test]\n",
    "\n",
    "x_train = tokenizer(x_train, return_tensors=\"tf\", padding=True, truncation=True,max_length=512)\n",
    "x_val = tokenizer(x_val, return_tensors=\"tf\", padding=True, truncation=True,max_length=512)\n",
    "x_test = tokenizer(x_test, return_tensors=\"tf\", padding=True, truncation=True,max_length=512)\n",
    "\n",
    "y_train = tf.convert_to_tensor(np.array(y_train), dtype=tf.int32)\n",
    "y_val = tf.convert_to_tensor(np.array(y_val), dtype=tf.int32)\n",
    "y_test = tf.convert_to_tensor(np.array(y_test), dtype=tf.int32)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), # or BinaryCrossentropy\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),tf.keras.metrics.AUC(name=\"auc\")]  #auc등 존재\n",
    ")\n",
    "\n",
    "# 콜백\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # 검증 손실 모니터링, val_auc도 가능\n",
    "    patience=3,          # 3번 연속 개선 없으면 종료\n",
    "    restore_best_weights=True)\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='DNA_BERT.keras',  # 저장할 파일 이름\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True)  \n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(x_train), y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((dict(x_val), y_val))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(x_test), y_test))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(100).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.shuffle(100).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.shuffle(100).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=3,\n",
    "    callbacks=[early_stop, model_checkpoint] \n",
    ")\n",
    "\n",
    "#저장\n",
    "model.save_pretrained(\"./saved_model\")\n",
    "tokenizer.save_pretrained(\"./saved_model\")\n",
    "\n",
    "# 평가\n",
    "loss, acc,auc = model.evaluate(val_dataset)\n",
    "\n",
    "#예측\n",
    "def predict_bert(model, tokenizer, texts, k=6):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    texts = [kmer_tokenizer(seq, k) for seq in texts]\n",
    "    enc = tokenizer(texts, return_tensors=\"tf\", padding=True, truncation=True,max_length=512)\n",
    "    logits = model(enc).logits\n",
    "    preds = tf.argmax(tf.nn.softmax(logits, axis=-1), axis=-1)\n",
    "    return preds.numpy()\n",
    "\n",
    "predict_bert(model,tokenizer,texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2dba5d-a400-46e9-a1d5-51d3c6a67a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc_curve그리기\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "\n",
    "# 1) val_dataset에서 예측 확률 얻기\n",
    "y_true = []\n",
    "y_scores = []\n",
    "\n",
    "for batch in val_dataset:\n",
    "    inputs, labels = batch\n",
    "    logits = model(inputs).logits\n",
    "    probs = tf.nn.softmax(logits, axis=1)[:, 1]  # 클래스 1에 대한 확률\n",
    "    y_scores.extend(probs.numpy())\n",
    "    y_true.extend(labels.numpy())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_scores = np.array(y_scores)\n",
    "\n",
    "# 2) ROC curve 계산\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# 3) ROC curve 시각화\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0,1], [0,1], color='navy', lw=1, linestyle='--')\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fce7fc-3de3-4cf9-99d9-ca96859eb62f",
   "metadata": {},
   "source": [
    "# 슬라이딩 윈도우 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390d33dc-6c37-4a35-b76e-b3556a18e216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. k-mer tokenizer\n",
    "def kmer_tokenizer(seq, k=6):\n",
    "    return ' '.join([seq[i:i+k] for i in range(len(seq) - k + 1)])\n",
    "\n",
    "# 2. 슬라이딩 윈도우 함수\n",
    "def sliding_window(seq, window_size=512 + 6 - 1, step=50):\n",
    "    windows = []\n",
    "    for start in range(0, len(seq) - window_size + 1, step):\n",
    "        windows.append(seq[start:start+window_size])\n",
    "    if (len(seq) - window_size) % step != 0:\n",
    "        windows.append(seq[-window_size:])\n",
    "    return windows\n",
    "\n",
    "# 3. 긴 시퀀스 리스트를 윈도우 분할 후 k-mer 변환, 레이블 복제까지\n",
    "def prepare_dataset(sequences, labels, k=6, window_size=512, step=50):\n",
    "    all_windows = []\n",
    "    all_labels = []\n",
    "    for seq, label in zip(sequences, labels):\n",
    "        windows = sliding_window(seq, window_size=window_size + k - 1, step=step)\n",
    "        kmer_windows = [kmer_tokenizer(w, k) for w in windows]\n",
    "        all_windows.extend(kmer_windows)\n",
    "        all_labels.extend([label] * len(windows))  # 각 윈도우에 원본 시퀀스 라벨 복제\n",
    "    return all_windows, all_labels\n",
    "\n",
    "# 4. 데이터 로드 및 분할 (x, y는 유전자 서열 리스트와 레이블 리스트)\n",
    "# 예: x = ['ACGT...', 'TGCA...', ...], y = [0, 1, ...]\n",
    "x_train_full, x_test, y_train_full, y_test = train_test_split(x, y, test_size=0.2, random_state=123)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_full, y_train_full, test_size=0.2, random_state=123)\n",
    "\n",
    "# 5. 윈도우 분할 및 k-mer 변환, 레이블 확장\n",
    "k = 6\n",
    "window_size = 512\n",
    "step = 50\n",
    "\n",
    "x_train_windows, y_train_windows = prepare_dataset(x_train, y_train, k, window_size, step)\n",
    "x_val_windows, y_val_windows = prepare_dataset(x_val, y_val, k, window_size, step)\n",
    "x_test_windows, y_test_windows = prepare_dataset(x_test, y_test, k, window_size, step)\n",
    "\n",
    "# 6. 토크나이저 및 모델 불러오기\n",
    "model_name = \"zhihan1996/DNABERT-6\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# 7. 토크나이징\n",
    "x_train_enc = tokenizer(x_train_windows, return_tensors=\"tf\", padding=True, truncation=True, max_length=window_size)\n",
    "x_val_enc = tokenizer(x_val_windows, return_tensors=\"tf\", padding=True, truncation=True, max_length=window_size)\n",
    "x_test_enc = tokenizer(x_test_windows, return_tensors=\"tf\", padding=True, truncation=True, max_length=window_size)\n",
    "\n",
    "# 8. 레이블 tensor 변환\n",
    "y_train_tensor = tf.convert_to_tensor(np.array(y_train_windows), dtype=tf.int32)\n",
    "y_val_tensor = tf.convert_to_tensor(np.array(y_val_windows), dtype=tf.int32)\n",
    "y_test_tensor = tf.convert_to_tensor(np.array(y_test_windows), dtype=tf.int32)\n",
    "\n",
    "# 9. 데이터셋 생성 및 배치 처리\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(x_train_enc), y_train_tensor)).shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((dict(x_val_enc), y_val_tensor)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(x_test_enc), y_test_tensor)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 10. 모델 컴파일\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"acc\"), tf.keras.metrics.AUC(name=\"auc\")]\n",
    ")\n",
    "\n",
    "# 11. 콜백 설정\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='DNA_BERT.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# 12. 모델 학습\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=3, callbacks=[early_stop, model_checkpoint])\n",
    "\n",
    "# 13. 평가\n",
    "results = model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {results[0]:.4f}, Accuracy: {results[1]:.4f}, AUC: {results[2]:.4f}\")\n",
    "\n",
    "# 14. 예측\n",
    "def predict_long_sequence(model, tokenizer, sequence, k=6, window_size=512, step=50):\n",
    "    windows = sliding_window(sequence, window_size=window_size + k - 1, step=step)\n",
    "    probs = []\n",
    "    for win_seq in windows:\n",
    "        kmer_seq = kmer_tokenizer(win_seq, k)\n",
    "        enc = tokenizer(kmer_seq, return_tensors=\"tf\", padding=True, truncation=True, max_length=window_size)\n",
    "        logits = model(enc).logits\n",
    "        prob = tf.nn.softmax(logits, axis=-1)[0, 1].numpy()  # 클래스 1 확률\n",
    "        probs.append(prob)\n",
    "    avg_prob = sum(probs) / len(probs)\n",
    "    return avg_prob, probs\n",
    "avg_prob, window_probs = predict_long_sequence(model, tokenizer, long_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc871915-dfea-44a4-825f-2187734deace",
   "metadata": {},
   "source": [
    "# 내부에서 각 샘플별 윈도우들 하나로 합쳐 분류층에 넣기\n",
    "- (샘플,윈도우,임베딩)을 (샘플X윈도우,임베딩)해서 bert에 넣고 (샘플X윈도우,cls)로 출력해서 (샘플,윈도우,cls)로 출력하고 같은 샘플의 윈도우끼리 cls 평균내서 (샘플,평균cls)로 변환후 dense에 넣음\n",
    "- mean, dense말고 max나 attention, lstm에도 넣을수있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd5eb4f-c857-4506-9e95-b23481902683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertModel, BertTokenizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#데이터\n",
    "x # 유전자 서열\n",
    "y # 레이블 값(0,1)\n",
    "\n",
    "# k-mer tokenizer\n",
    "def kmer_tokenizer(seq, k=6):\n",
    "    return ' '.join([seq[i:i+k] for i in range(len(seq) - k + 1)])\n",
    "\n",
    "# sliding window\n",
    "def sliding_window(seq, window_size=512 + 6 - 1, step=50):\n",
    "    windows = []\n",
    "    for start in range(0, len(seq) - window_size + 1, step):\n",
    "        windows.append(seq[start:start+window_size])\n",
    "    if (len(seq) - window_size) % step != 0:\n",
    "        windows.append(seq[-window_size:])\n",
    "    return windows\n",
    "\n",
    "# 모델 클래스 정의\n",
    "class WindowAggregateClassifier(tf.keras.Model):\n",
    "    def __init__(self, pretrained_model_name, num_labels=2):\n",
    "        super().__init__()\n",
    "        self.bert = TFBertModel.from_pretrained(pretrained_model_name)\n",
    "        self.classifier = tf.keras.layers.Dense(num_labels)  # 분류기, logits 출력\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        inputs: dict with keys:\n",
    "            - input_ids: shape (batch_size, windows, seq_len)\n",
    "            - attention_mask: shape (batch_size, windows, seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        input_ids = inputs['input_ids']   # (batch, windows, seq_len)\n",
    "        attention_mask = inputs['attention_mask']  # (batch, windows, seq_len)\n",
    "\n",
    "        batch_size = tf.shape(input_ids)[0]\n",
    "        windows = tf.shape(input_ids)[1]\n",
    "        seq_len = tf.shape(input_ids)[2]\n",
    "\n",
    "        # windows 차원과 batch 차원을 합쳐서 BERT에 한번에 넣기\n",
    "        input_ids_reshaped = tf.reshape(input_ids, (-1, seq_len))          # (batch*windows, seq_len)\n",
    "        attention_mask_reshaped = tf.reshape(attention_mask, (-1, seq_len))# (batch*windows, seq_len)\n",
    "\n",
    "        bert_outputs = self.bert(input_ids_reshaped, attention_mask=attention_mask_reshaped, training=training)\n",
    "        # pooled_output: (batch*windows, hidden_size)\n",
    "        pooled_output = bert_outputs.pooler_output\n",
    "\n",
    "        # 다시 (batch, windows, hidden_size)로 reshape\n",
    "        pooled_output = tf.reshape(pooled_output, (batch_size, windows, -1))\n",
    "\n",
    "        # 윈도우 임베딩 평균\n",
    "        pooled_mean = tf.reduce_mean(pooled_output, axis=1)  # (batch, hidden_size)\n",
    "\n",
    "        logits = self.classifier(pooled_mean)  # (batch, num_labels)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# 토크나이저 및 하이퍼파라미터\n",
    "model_name = \"zhihan1996/DNABERT-6\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "num_labels = 2\n",
    "\n",
    "# 예시 입력 데이터 (batch=2, 각 시퀀스 길이 길어서 윈도우 3개씩)\n",
    "example_sequences = [\n",
    "    \"ACGTGCTAGCTAGCTAGCTGATCGATCGTACGATCGATGCTAGCTAGCTAGCATCGATCGATGCTAGCTAGCTAGCATCGATCGATGC\",\n",
    "    \"TGCTAGCTAGCTAGCTAGCTAGCATCGATGCTAGCTAGCTAGCATCGATCGTAGCTAGCTAGCATCGTAGCTAGCTAGCTAGCATCGA\"\n",
    "]\n",
    "\n",
    "# 1. 각 시퀀스 윈도우 분할 및 k-mer 토크나이징\n",
    "k = 6\n",
    "window_size = 512\n",
    "step = 50\n",
    "\n",
    "all_windows = []\n",
    "window_counts = []\n",
    "for seq in example_sequences:\n",
    "    windows = sliding_window(seq, window_size=window_size + k - 1, step=step)\n",
    "    window_counts.append(len(windows))\n",
    "    windows_kmer = [kmer_tokenizer(w, k) for w in windows]\n",
    "    all_windows.extend(windows_kmer)\n",
    "\n",
    "# 2. 토크나이징 한꺼번에\n",
    "encodings = tokenizer(all_windows, padding='max_length', truncation=True, max_length=window_size, return_tensors='tf')\n",
    "\n",
    "# 3. 배치 및 윈도우 개수 맞춰서 reshape\n",
    "max_windows = max(window_counts)\n",
    "input_ids = encodings['input_ids']\n",
    "attention_mask = encodings['attention_mask']\n",
    "\n",
    "# 윈도우 개수가 다르면 패딩 필요 → 짧은 시퀀스는 빈 윈도우로 패딩\n",
    "def pad_windows(input_ids, attention_mask, window_counts, max_windows):\n",
    "    padded_input_ids = []\n",
    "    padded_attention_mask = []\n",
    "    start = 0\n",
    "    seq_len = input_ids.shape[1]\n",
    "    for count in window_counts:\n",
    "        windows_slice = input_ids[start:start+count]\n",
    "        mask_slice = attention_mask[start:start+count]\n",
    "        pad_len = max_windows - count\n",
    "        if pad_len > 0:\n",
    "            windows_pad = tf.zeros((pad_len, seq_len), dtype=input_ids.dtype)\n",
    "            mask_pad = tf.zeros((pad_len, seq_len), dtype=attention_mask.dtype)\n",
    "            windows_slice = tf.concat([windows_slice, windows_pad], axis=0)\n",
    "            mask_slice = tf.concat([mask_slice, mask_pad], axis=0)\n",
    "        padded_input_ids.append(windows_slice)\n",
    "        padded_attention_mask.append(mask_slice)\n",
    "        start += count\n",
    "    return tf.stack(padded_input_ids), tf.stack(padded_attention_mask)\n",
    "\n",
    "input_ids_padded, attention_mask_padded = pad_windows(input_ids, attention_mask, window_counts, max_windows)\n",
    "\n",
    "# 4. 모델 생성 및 컴파일\n",
    "model = WindowAggregateClassifier(pretrained_model_name=model_name, num_labels=num_labels)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy(name='acc'), tf.keras.metrics.AUC(name='auc')]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# 5. 레이블 (예시)\n",
    "labels = tf.constant([0,1])  # 배치 사이즈 2\n",
    "\n",
    "# 6. 학습 예시 (여기서는 그냥 한 번 호출)\n",
    "logits = model({'input_ids': input_ids_padded, 'attention_mask': attention_mask_padded})\n",
    "print(logits)\n",
    "\n",
    "# 7. fit에 맞게 데이터셋 만들려면 input dict 형식 맞춰서 구성하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad9c66-bea6-4fcb-a5a2-7d40b6c0c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측\n",
    "def predict_long_sequence_class(model, tokenizer, sequence, k=6, window_size=512, step=50):\n",
    "    # 1. 슬라이딩 윈도우 → k-mer 변환\n",
    "    windows = sliding_window(sequence, window_size + k - 1, step)\n",
    "    kmer_windows = [kmer_tokenizer(w, k) for w in windows]\n",
    "\n",
    "    # 2. 토크나이징\n",
    "    encodings = tokenizer(kmer_windows, padding='max_length', truncation=True,\n",
    "                          max_length=window_size, return_tensors='tf')\n",
    "    \n",
    "    input_ids = encodings['input_ids']\n",
    "    attention_mask = encodings['attention_mask']\n",
    "\n",
    "    # 3. 윈도우 개수 맞춰서 패딩 (batch=1)\n",
    "    input_ids_padded, attention_mask_padded = pad_windows(input_ids, attention_mask, [len(windows)], len(windows))\n",
    "\n",
    "    # 4. 예측\n",
    "    logits = model({'input_ids': input_ids_padded, 'attention_mask': attention_mask_padded})\n",
    "    probs = tf.nn.softmax(logits, axis=-1)\n",
    "    pred_class = tf.argmax(probs, axis=-1).numpy()[0]      # 예측된 클래스\n",
    "    pred_prob = probs.numpy()[0][1]                        # 클래스 1일 확률\n",
    "\n",
    "    return pred_class, pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dab77f0-b665-4c8e-9a94-45abd455404e",
   "metadata": {},
   "source": [
    "# 최종"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79448be4-a81a-48cf-8a97-02c07156483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertModel, BertTokenizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ============================\n",
    "# 1. 데이터\n",
    "# ============================\n",
    "x = [...]  # 유전자 서열 리스트 (문자열)\n",
    "y = [...]  # 정답 (0 또는 1)\n",
    "\n",
    "# ============================\n",
    "# 2. 전처리 함수\n",
    "# ============================\n",
    "def kmer_tokenizer(seq, k=6):\n",
    "    return ' '.join([seq[i:i+k] for i in range(len(seq) - k + 1)])\n",
    "\n",
    "def sliding_window(seq, window_size=512+6-1, step=300):\n",
    "    windows = []\n",
    "    for start in range(0, len(seq) - window_size + 1, step):\n",
    "        windows.append(seq[start:start+window_size])\n",
    "    if (len(seq) - window_size) % step != 0:\n",
    "        windows.append(seq[-window_size:])\n",
    "    return windows\n",
    "\n",
    "def encode_and_pad(sequences, labels, tokenizer, k=6, window_size=512, step=300):\n",
    "    max_seq_len = max(len(seq) for seq in sequences)\n",
    "    max_windows = (max_seq_len - (window_size + k - 1)) // step + 2  # +2 여유 패딩용\n",
    "\n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "    all_labels = []\n",
    "\n",
    "    for seq, label in zip(sequences, labels):\n",
    "        windows = sliding_window(seq, window_size + k - 1, step)\n",
    "        # max_windows 이상이면 자르기 (필요하면)\n",
    "        if len(windows) > max_windows:\n",
    "            windows = windows[:max_windows]\n",
    "\n",
    "        kmer_windows = [kmer_tokenizer(w, k) for w in windows]\n",
    "        enc = tokenizer(kmer_windows, padding='max_length', truncation=True,\n",
    "                        max_length=window_size, return_tensors='np')\n",
    "\n",
    "        input_ids = enc['input_ids']\n",
    "        attention_mask = enc['attention_mask']\n",
    "\n",
    "        pad_len = max_windows - input_ids.shape[0]\n",
    "        if pad_len > 0:\n",
    "            input_ids = np.pad(input_ids, ((0, pad_len), (0, 0)), constant_values=0)\n",
    "            attention_mask = np.pad(attention_mask, ((0, pad_len), (0, 0)), constant_values=0)\n",
    "\n",
    "        all_input_ids.append(input_ids)\n",
    "        all_attention_masks.append(attention_mask)\n",
    "        all_labels.append(label)\n",
    "\n",
    "    return (\n",
    "        tf.convert_to_tensor(all_input_ids, dtype=tf.int32),\n",
    "        tf.convert_to_tensor(all_attention_masks, dtype=tf.int32),\n",
    "        tf.convert_to_tensor(all_labels, dtype=tf.int32)\n",
    "    )\n",
    "\n",
    "# ============================\n",
    "# 3. 모델 클래스 정의\n",
    "# ============================\n",
    "class WindowAggregateClassifier(tf.keras.Model):\n",
    "    def __init__(self, pretrained_model_name, num_labels=2):\n",
    "        super().__init__()\n",
    "        self.bert = TFBertModel.from_pretrained(pretrained_model_name)\n",
    "        self.classifier = tf.keras.layers.Dense(num_labels)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        batch_size = tf.shape(input_ids)[0]\n",
    "        windows = tf.shape(input_ids)[1]\n",
    "        seq_len = tf.shape(input_ids)[2]\n",
    "\n",
    "        input_ids_flat = tf.reshape(input_ids, (-1, seq_len))\n",
    "        attn_flat = tf.reshape(attention_mask, (-1, seq_len))\n",
    "\n",
    "        bert_output = self.bert(input_ids_flat, attention_mask=attn_flat, training=training)\n",
    "        pooled = bert_output.pooler_output  # (batch * windows, hidden)\n",
    "\n",
    "        pooled = tf.reshape(pooled, (batch_size, windows, -1))\n",
    "        mean_pooled = tf.reduce_mean(pooled, axis=1)  # (batch, hidden)\n",
    "\n",
    "        logits = self.classifier(mean_pooled)  # (batch, num_labels)\n",
    "        return logits\n",
    "\n",
    "# ============================\n",
    "# 4. 학습/검증/테스트 분할\n",
    "# ============================\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# ============================\n",
    "# 5. 모델 및 토크나이저 준비\n",
    "# ============================\n",
    "model_name = \"zhihan1996/DNABERT-6\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = WindowAggregateClassifier(pretrained_model_name=model_name, num_labels=2)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(2e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "             tf.keras.metrics.AUC(name=\"auc\")]\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='DNA_BERT.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# ============================\n",
    "# 6. Dataset 생성\n",
    "# ============================\n",
    "def make_dataset(x_data, y_data, tokenizer, batch_size=32):\n",
    "    input_ids, attn_mask, labels = encode_and_pad(x_data, y_data, tokenizer)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(({\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attn_mask\n",
    "    }, labels))\n",
    "    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds = make_dataset(x_train, y_train, tokenizer)\n",
    "val_ds   = make_dataset(x_val, y_val, tokenizer)\n",
    "test_ds  = make_dataset(x_test, y_test, tokenizer)\n",
    "\n",
    "# ============================\n",
    "# 7. 학습\n",
    "# ============================\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=10, callbacks=[early_stop, model_checkpoint])\n",
    "\n",
    "# ============================\n",
    "# 8. 저장\n",
    "# ============================\n",
    "model.save_weights(\"dna_bert_classifier.weights.h5\")\n",
    "tokenizer.save_pretrained(\"./saved_tokenizer\")\n",
    "\n",
    "# ============================\n",
    "# 9. 평가\n",
    "# ============================\n",
    "model.evaluate(test_ds)\n",
    "\n",
    "# ============================\n",
    "# 10. 예측 함수\n",
    "# ============================\n",
    "def predict_sequence(model, tokenizer, sequence, k=6, window_size=512, step=300, max_windows=120):\n",
    "    # 1. 슬라이딩 윈도우 분할 (step=300으로 바꿨음)\n",
    "    windows = sliding_window(sequence, window_size + k - 1, step)[:max_windows]\n",
    "    \n",
    "    # 2. k-mer 토크나이징\n",
    "    kmer_windows = [kmer_tokenizer(w, k) for w in windows]\n",
    "\n",
    "    # 3. 토크나이징 및 패딩\n",
    "    enc = tokenizer(kmer_windows, padding='max_length', truncation=True,\n",
    "                    max_length=window_size, return_tensors='tf')\n",
    "\n",
    "    input_ids = enc['input_ids']\n",
    "    attn_mask = enc['attention_mask']\n",
    "\n",
    "    # 4. 윈도우 부족하면 0패딩\n",
    "    pad_len = max_windows - input_ids.shape[0]\n",
    "    if pad_len > 0:\n",
    "        input_ids = tf.pad(input_ids, [[0, pad_len], [0, 0]])\n",
    "        attn_mask = tf.pad(attn_mask, [[0, pad_len], [0, 0]])\n",
    "\n",
    "    # 5. 배치 차원 추가\n",
    "    input_ids = tf.expand_dims(input_ids, axis=0)        # (1, windows, seq_len)\n",
    "    attn_mask = tf.expand_dims(attn_mask, axis=0)\n",
    "\n",
    "    # 6. 예측\n",
    "    logits = model({'input_ids': input_ids, 'attention_mask': attn_mask}, training=False)\n",
    "    probs = tf.nn.softmax(logits, axis=-1)\n",
    "    pred = tf.argmax(probs, axis=-1).numpy()[0]\n",
    "\n",
    "    return int(pred), probs.numpy()[0]\n",
    "\n",
    "## 예측하기\n",
    "long_seq = \"ACGT\" * 10000  # 길이 40000짜리 유전자 서열\n",
    "pred, prob = predict_sequence(model, tokenizer, long_seq)\n",
    "print(f\"예측 클래스: {pred}, 확률 분포: {prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7138c258-4c27-4a09-af99-65c06806e288",
   "metadata": {},
   "source": [
    "### attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b3aed1-9d5e-401f-957d-42e0add7eddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention버전\n",
    "class WindowAttentionClassifier(tf.keras.Model):\n",
    "    def __init__(self, pretrained_model_name, num_labels=2, hidden_size=768):\n",
    "        super().__init__()\n",
    "        self.bert = TFBertModel.from_pretrained(pretrained_model_name)\n",
    "        self.attn_layer = tf.keras.layers.Dense(1)  # attention weight 계산\n",
    "        self.classifier = tf.keras.layers.Dense(num_labels)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_ids = inputs['input_ids']         # (batch, windows, seq_len)\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        batch_size = tf.shape(input_ids)[0]\n",
    "        num_windows = tf.shape(input_ids)[1]\n",
    "        seq_len = tf.shape(input_ids)[2]\n",
    "\n",
    "        input_ids = tf.reshape(input_ids, (-1, seq_len))            # (batch * windows, seq_len)\n",
    "        attention_mask = tf.reshape(attention_mask, (-1, seq_len))  # (batch * windows, seq_len)\n",
    "\n",
    "        bert_outputs = self.bert(input_ids, attention_mask=attention_mask, training=training)\n",
    "        pooled_output = bert_outputs.pooler_output  # (batch * windows, hidden_size)\n",
    "\n",
    "        window_embeddings = tf.reshape(pooled_output, (batch_size, num_windows, -1))  # (batch, windows, hidden)\n",
    "\n",
    "        # Attention weights: (batch, windows, 1)\n",
    "        attn_logits = self.attn_layer(window_embeddings)\n",
    "        attn_weights = tf.nn.softmax(attn_logits, axis=1)\n",
    "\n",
    "        # 가중합: (batch, hidden)\n",
    "        weighted_sum = tf.reduce_sum(attn_weights * window_embeddings, axis=1)\n",
    "\n",
    "        logits = self.classifier(weighted_sum)  # (batch, num_labels)\n",
    "        return logits\n",
    "'''\n",
    "window_embeddings는 이미 BERT를 통과한 벡터들 → 즉, V (value) 역할\n",
    "\n",
    "self.attn_layer(Dense(1))는 각 윈도우 벡터에 대해 스칼라 score 계산 → attention 점수\n",
    "\n",
    "softmax는 이 score를 확률처럼 정규화 → attention weight\n",
    "\n",
    "attn_weights * window_embeddings는 V에 대한 가중치 곱\n",
    "\n",
    "reduce_sum은 attention-weighted sum → 최종 attention pooled 벡터\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ef63c-a234-434d-b51c-b68580d30f33",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c235442-9a79-4235-8c64-21d0b001b869",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowLSTMClassifier(tf.keras.Model):\n",
    "    def __init__(self, pretrained_model_name, num_labels=2, hidden_size=768):\n",
    "        super().__init__()\n",
    "        self.bert = TFBertModel.from_pretrained(pretrained_model_name)\n",
    "        self.lstm = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(hidden_size, return_sequences=False)\n",
    "        )\n",
    "        self.classifier = tf.keras.layers.Dense(num_labels)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_ids = inputs['input_ids']         # (batch, windows, seq_len)\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        batch_size = tf.shape(input_ids)[0]\n",
    "        num_windows = tf.shape(input_ids)[1]\n",
    "        seq_len = tf.shape(input_ids)[2]\n",
    "\n",
    "        input_ids = tf.reshape(input_ids, (-1, seq_len))\n",
    "        attention_mask = tf.reshape(attention_mask, (-1, seq_len))\n",
    "\n",
    "        bert_outputs = self.bert(input_ids, attention_mask=attention_mask, training=training)\n",
    "        pooled_output = bert_outputs.pooler_output  # (batch * windows, hidden_size)\n",
    "\n",
    "        window_embeddings = tf.reshape(pooled_output, (batch_size, num_windows, -1))  # (batch, windows, hidden)\n",
    "\n",
    "        lstm_output = self.lstm(window_embeddings)  # (batch, hidden*2)\n",
    "\n",
    "        logits = self.classifier(lstm_output)  # (batch, num_labels)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987b459a-197e-4932-903f-9c7a06f43684",
   "metadata": {},
   "source": [
    "# DNA_BERT+MIL: 위의 attention과 거의 유사한 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84704191-5fc4-42ae-99f9-7b9cff2995da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertModel, BertTokenizerFast\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 1. DNA-BERT 모델과 토크나이저 불러오기\n",
    "bert_model_name = \"zhihan1996/DNA_bert_6\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(bert_model_name)\n",
    "bert = TFBertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "# 2. Sliding window 함수 (k-mer=6 기준)\n",
    "def sliding_windows(sequence, k=6, window_size=512, stride=128):\n",
    "    kmers = [sequence[i:i+k] for i in range(len(sequence) - k + 1)]\n",
    "    windows = []\n",
    "    for start in range(0, len(kmers) - window_size + 1, stride):\n",
    "        window = \" \".join(kmers[start:start+window_size])\n",
    "        windows.append(window)\n",
    "    return windows\n",
    "\n",
    "# 3. Attention Pooling Layer (MIL에서 optional)\n",
    "class AttentionPooling(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.dense = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs: [batch, num_windows, hidden_dim]\n",
    "        scores = tf.nn.softmax(self.dense(inputs), axis=1)  # 윈도우별 attention score\n",
    "        weighted_sum = tf.reduce_sum(inputs * scores, axis=1)\n",
    "        return weighted_sum  # [batch, hidden_dim]\n",
    "\n",
    "# 4. MIL 모델 빌드 함수\n",
    "def build_dnabert_mil_classifier(bert, hidden_dim=768, pooling=\"max\"):\n",
    "    # 입력: batch 크기 상관없이 윈도우 개수 변동 가능 (None)\n",
    "    input_ids = tf.keras.Input(shape=(None, 512), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = tf.keras.Input(shape=(None, 512), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "    # 윈도우별 BERT 임베딩 추출\n",
    "    def get_cls_embedding(i):\n",
    "        outputs = bert(input_ids[:, i], attention_mask=attention_mask[:, i])\n",
    "        return outputs.last_hidden_state[:, 0]  # [CLS] 토큰 벡터 [batch, hidden_dim]\n",
    "\n",
    "    # 윈도우 개수\n",
    "    num_windows = tf.shape(input_ids)[1]\n",
    "\n",
    "    # 모든 윈도우에 대해 CLS 임베딩 계산 (tf.map_fn 사용)\n",
    "    cls_embeddings = tf.map_fn(get_cls_embedding, tf.range(num_windows), dtype=tf.float32)\n",
    "    cls_embeddings = tf.transpose(cls_embeddings, perm=[1, 0, 2])  # [batch, num_windows, hidden_dim]\n",
    "\n",
    "    # MIL Pooling (max, mean, attention 중 선택)\n",
    "    if pooling == \"max\":\n",
    "        pooled = tf.reduce_max(cls_embeddings, axis=1)\n",
    "    elif pooling == \"mean\":\n",
    "        pooled = tf.reduce_mean(cls_embeddings, axis=1)\n",
    "    elif pooling == \"attention\":\n",
    "        pooled = AttentionPooling(hidden_dim)(cls_embeddings)\n",
    "    else:\n",
    "        raise ValueError(\"pooling must be 'max', 'mean', or 'attention'\")\n",
    "\n",
    "    # 최종 분류 레이어 (이진 분류)\n",
    "    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(pooled)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "    return model\n",
    "\n",
    "# 5. 실제 사용 예시\n",
    "\n",
    "# 긴 DNA 서열 (예시)\n",
    "sequence = \"ATGCGTACGTTAGCTAGCTAGCTGATCGTACGATCGTAGCTAGCTAGCTAGCTA\" * 1000  # 5만 bp 이상\n",
    "\n",
    "# 윈도우 생성\n",
    "windows = sliding_windows(sequence, k=6, window_size=512, stride=128)\n",
    "print(f\"윈도우 개수: {len(windows)}\")\n",
    "\n",
    "# 토크나이징 (batch size=1 가정, padding/truncation 필요)\n",
    "tokens = tokenizer(windows, return_tensors=\"tf\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# 입력 차원 맞추기\n",
    "input_ids = tf.expand_dims(tokens[\"input_ids\"], axis=0)       # [batch=1, num_windows, seq_len]\n",
    "attention_mask = tf.expand_dims(tokens[\"attention_mask\"], axis=0)\n",
    "\n",
    "# 모델 생성\n",
    "model = build_dnabert_mil_classifier(bert, pooling=\"attention\")  # pooling: max/mean/attention 선택 가능\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# 가짜 레이블 (positive=1, batch size=1)\n",
    "y = np.array([1])\n",
    "\n",
    "# 학습 예시 (실제 데이터셋에서는 batch 처리 권장)\n",
    "model.fit([input_ids, attention_mask], y, epochs=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
