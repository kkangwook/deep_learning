{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c9449eb-2af4-4d5e-98d1-948e3626ce2f",
   "metadata": {},
   "source": [
    "# 코랩에서 TPU사용하기\n",
    "- Colab에서 런타임 > 런타임 유형 변경 > 하드웨어 가속기에서 'TPU' 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9792620b-8979-419e-a0d7-1d01e87ecfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPU초기화\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "\n",
    "#  TPU Strategy 셋팅\n",
    "strategy = tf.distribute.TPUStrategy(resolver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e1031e-7c45-4d72-a083-264c4446100d",
   "metadata": {},
   "source": [
    "### 딥 러닝 모델을 컴파일을 할 때도 추가적인 코드가 필요\n",
    "- 모델의 컴파일은 strategy.scope 내에서 이루어져야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa5db7a-d5e8-4366-be3f-cda6f0a94a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 쌓기\n",
    "def create_model():\n",
    "  return tf.keras.Sequential(\n",
    "      [tf.keras.layers.Conv2D(256, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "       tf.keras.layers.Conv2D(256, 3, activation='relu'),\n",
    "       tf.keras.layers.Flatten(),\n",
    "       tf.keras.layers.Dense(256, activation='relu'),\n",
    "       tf.keras.layers.Dense(128, activation='relu'),\n",
    "       tf.keras.layers.Dense(10)])\n",
    "\n",
    "# 컴파일\n",
    "with strategy.scope():\n",
    "  model = create_model()\n",
    "  model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "#이후 fit()하면 TPU버전으로"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21232ad2-82ca-44b8-b936-34590bc208ba",
   "metadata": {},
   "source": [
    "## 안끊기게 하기\n",
    "- F12 -> 콘솔탭에 밑에꺼 붙여넣기 -> F12창 계속 켜놯야함  \n",
    "function ClickConnect(){  \n",
    "console.log(\"Working\");   \n",
    "document.querySelector(\"colab-toolbar-button#connect\").click()   \n",
    "}setInterval(ClickConnect, 1800000)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b89e070-ecc0-4d5e-be77-e5a993b9dfde",
   "metadata": {},
   "source": [
    "# BERT 기본 사용법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d014f2-2308-4dda-b688-9c9465aa70c3",
   "metadata": {},
   "source": [
    "# 1. 토크나이징하기\n",
    "- BertTokenizer, RobertaTokenizer, GPT2Tokenizer, AlbertTokenizer, AutoTokenizer등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11aebc5-ab23-400f-b758-f9e1225d008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임포트\n",
    "from transformers import BertTokenizer # bert용 wordpiece기반 : [CLS]-단어-[SEP]\n",
    "from transformers import BertTokenizerFast # 빠른버전\n",
    "from transformers import AutoTokenizer # 추천!!! 거의 모든 종류에 사용가능(자동으로 추천해줌)\n",
    "\n",
    "# 사전학습정보 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") # 이 안에 이름\n",
    "\n",
    "# 토크나이징 하기\n",
    "encoded = tokenizer(text,                 #하나의 문자열 or 문자들의 list\n",
    "                    padding='max_length', # True 또는 'longest': 가장 긴 길이로, max_length: 안에 따로 max_length=n파라미터 필요, False: 안함\n",
    "                    truncation=True, # 지정한 padding값이나 512길이가 넘어가면 자름(앞에서부터)\n",
    "                    max_length=128, # 패딩 길이\n",
    "                    return_tensors='tf') # tf:텐서플로우 들어갈때, pt: pytorch들어갈때, np: 넘파이 형태로 반환\n",
    "\n",
    "# 디코딩하기(숫자를 다시 원래 단어로)\n",
    "decoded = tokenizer.decode(encoded['input_ids'][n].numpy().tolist(), # n-1번째 샘플 디코딩 -> tf형태이므로 넘파이로 바꾸고 다시 리스트로\n",
    "                           skip_special_tokens=True) # 특수토큰 제외"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5874e9-a593-4bdc-aed2-7392851009fc",
   "metadata": {},
   "source": [
    "### encoded값\n",
    "{  \n",
    "  'input_ids': [...], # 값이 숫자인덱스로 변환된 데이터  \n",
    "  'attention_mask': [...], # 의미있는 토큰=1, 패딩된 토큰은 0  \n",
    "  'token_type_ids': [...]  # 각 문장을 0부터 1씩 증가하게 표현, 문장쌍일 때만 의미 있음  \n",
    "}  \n",
    "\n",
    "### 예시\n",
    "{  \n",
    "  'input_ids': [  \n",
    "    [101, 1045, 2293, 102, 0, 0],        # 첫 번째 샘플의 토큰 ID 시퀀스  \n",
    "    [101, 2026, 2171, 2003, 102, 0],    # 두 번째 샘플  \n",
    "    [101, 2009, 2001, 2204, 102, 0]     # 세 번째 샘플  \n",
    "  ],  \n",
    "  'attention_mask': [  \n",
    "    [1, 1, 1, 1, 0, 0],  # 첫 번째 샘플: 마지막 두 개는 패딩이라 0  \n",
    "    [1, 1, 1, 1, 1, 0],  # 두 번째 샘플  \n",
    "    [1, 1, 1, 1, 1, 0]   # 세 번째 샘플  \n",
    "  ],  \n",
    "  'token_type_ids': [  \n",
    "    [0, 0, 0, 0, 0, 0],  # 문장 단일 입력이라 모두 0  \n",
    "    [0, 0, 0, 0, 0, 0],  \n",
    "    [0, 0, 0, 0, 0, 0]  \n",
    "  ]  \n",
    "}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84953a58-f073-491f-8b80-75eafd39a0e0",
   "metadata": {},
   "source": [
    "# 2. BERT여러 모델들\n",
    "- 파이토치용은 그냥 Bert~\n",
    "- 텐서플로우용은 TFBert~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29723fa-5899-46b3-bbd1-2805217e74e9",
   "metadata": {},
   "source": [
    "### 2-1. BERT기본 모델: TFBertModel\n",
    "- tokenizer의 dict형태 그대로 넣거나 !!!!!!\n",
    "- input_ids와 attention_mask 두개만 넣어도 됌!!!!!!!!!!!! (token_type_ids 디폴트는 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62666c76-f7a6-4867-ab68-048ff44a8d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, BERT!\", return_tensors=\"tf\")\n",
    "\n",
    "# 토크나이징 된걸 그냥 model에 넣으면 됨\n",
    "outputs = model(inputs)\n",
    "\n",
    "# 기본 BERT의 마지막 층 출력\n",
    "last_hidden_states = outputs.last_hidden_state   #모든 토큰(cls,sep등 포함)에 대한 최종 히든 벡터: (batch_size, seq_len, hidden_size)\n",
    "pooler_output = outputs.pooler_output            #샘플별 맨앞 하나 존재하는 cls에 추가적인 층과 tanh활성화를 거친 벡터\n",
    "                                                 # -> 그 샘플의 문장을 대표하는 벡터: (batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828283e2-8507-43f9-94af-dbff2e6fe37d",
   "metadata": {},
   "source": [
    "### 2-2. TFBertForSequenceClassification(문장 분류용 층이 추가)\n",
    "- 입력(다) 대 출력(일)\n",
    "- 출력값은 클래스별 logits(z값)\n",
    "### cls!!!값을 추가 dense+여러 구성에 넣어 분류한것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdf9a42-2600-4790-b61d-3c67cb07b27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=n) # 이진분류면 1 or 2, 다중분류면 해당 클래스 개수 n\n",
    "\n",
    "inputs = tokenizer(\"Hello, BERT!\", return_tensors=\"tf\")\n",
    "outputs = model(inputs)\n",
    "\n",
    "#최종 출력층 값\n",
    "logits = outputs.logits   # (batch_size, num_labels)==(샘플수, n)\n",
    "\n",
    "# logits → 확률 변환 (softmax)\n",
    "probabilities = tf.nn.softmax(logits, axis=-1) # 마지막차원에 적용하는 축-> 각 클래스별 확률구함\n",
    "\n",
    "# 각 샘플별 예측 클래스 인덱스\n",
    "predicted_class = tf.argmax(probabilities, axis=-1).numpy()[0]\n",
    "\n",
    "#### num_labels=1일경우 시그모이드(이진분류는 num_labels=2하고 그냥 소프트맥스해도됨)\n",
    "# sigmoid로 확률 변환\n",
    "probability = tf.sigmoid(logits)[0][0].numpy()\n",
    "\n",
    "# 예측 클래스 (threshold 0.5)\n",
    "predicted_class = 1 if probability > 0.5 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e814c3-6f56-4911-a060-461f9c05da9c",
   "metadata": {},
   "source": [
    "## 이때 최종값으로 logits값이 나오지만 fit()할때는 compile에 loss값을 넣어주므로 SparseCategoricalCrossentropy, BinaryCrossentropy등에 의해 자동으로 확률-> 클래스로 변환해 정답값과 비교할수있게됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c897d6-822c-4fce-a339-86baec2a4442",
   "metadata": {},
   "source": [
    "### 2-3. TFBertForTokenClassification(토큰분류용)\n",
    "- 각 토큰마다의 레이블 예측(다 대 다)\n",
    "- 품사 태깅 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa05220-606a-4cae-a321-ab2de0ce5bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForTokenClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = TFBertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=n) # 각 토큰이 분류될수있는 클래스 수!!!!\n",
    "\n",
    "inputs = tokenizer(\"Hello, BERT!\", return_tensors=\"tf\")\n",
    "outputs = model(inputs)\n",
    "\n",
    "# logits[i][j]는 i번째 문장의 j번째 토큰\n",
    "logits = outputs.logits   # (batch_size, seq_len, num_labels) -> 문장의 각 토큰별 n개의 분류점수값\n",
    "\n",
    "# 각 토큰별 예측 클래스 보기\n",
    "predicted_classes = tf.argmax(logits, axis=-1)  #(batch_size, seq_len) 예시) 하나의 문장에서 [1,0,0,2,4,0,3]\n",
    "\n",
    "# 확률로 보기\n",
    "probs = tf.nn.softmax(logits, axis=-1)  # (batch, seq_len, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed59b1fc-570d-44b2-aff2-3f4d5fba48fc",
   "metadata": {},
   "source": [
    "### 2-4. TFBertForQuestionAnswering(질문 답변용 모델)\n",
    "- 입력 문장 내에서 정답의 시작/끝 위치 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3424ad-6834-451e-bed8-e9d32008c147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForQuestionAnswering\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = TFBertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "question = \"Who was Jim Henson?\"\n",
    "text = \"Jim Henson was a nice puppet\"\n",
    "\n",
    "inputs = tokenizer(question, text, return_tensors=\"tf\") # 질문, 정답이 들은 문맥으로 줌 -> [CLS] 질문 [SEP] 문맥 [SEP]형태\n",
    "outputs = model(inputs)\n",
    "\n",
    "start_logits = outputs.start_logits  # (batch_size, seq_len) :각 토큰이 \"답 시작\"일 logit 점수\n",
    "end_logits = outputs.end_logits      # (batch_size, seq_len) : 각 토큰이 \"답 끝\"일 logit 점수\n",
    "\n",
    "# 가장 높은 점수의 토큰 찾기\n",
    "start_index = tf.argmax(outputs.start_logits, axis=-1).numpy()[0]\n",
    "end_index = tf.argmax(outputs.end_logits, axis=-1).numpy()[0]\n",
    "\n",
    "# 토큰-> 원문단어로\n",
    "input_ids = inputs[\"input_ids\"][0]  # 1번째 샘플 토큰 ID 시퀀스\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids) # 숫자 리스트-> 문자열 리스트\n",
    "\n",
    "answer_tokens = tokens[start_index : end_index + 1]\n",
    "answer = tokenizer.convert_tokens_to_string(answer_tokens) # 문자열 리스트 -> 문자열로\n",
    "\n",
    "print(\"Predicted Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb4da3d-c4ef-4e8c-84bc-d07653994a0d",
   "metadata": {},
   "source": [
    "### 2-5. 커스텀 출력층 추가 (TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1cac06-3d26-45b6-a964-58ce43738922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertModel\n",
    "\n",
    "class TFBertClassifier(tf.keras.Model):\n",
    "    def __init__(self, num_labels=2):\n",
    "        super().__init__()\n",
    "        self.bert = TFBertModel.from_pretrained(\"bert-base-uncased\")  # 기본 모델\n",
    "        self.classifier = tf.keras.layers.Dense(num_labels)   # 추가해줄 층, 활성화 함수는 없음\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = self.bert(inputs)\n",
    "        pooled_output = outputs.pooler_output  # (batch_size, hidden_size), 처리된 cls벡터\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# 사용 예시\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = TFBertClassifier(num_labels=2)\n",
    "\n",
    "inputs = tokenizer(\"Hello TensorFlow BERT!\", return_tensors=\"tf\")\n",
    "logits = model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dbbbd3-e3f2-4459-8174-54ba8f3f47b3",
   "metadata": {},
   "source": [
    "# 3. 컴파일하기\n",
    "- 마지막 층이 활성화 함수 안거친 logits이면(activation=None) loss에 from_logits=True(자동으로 활성화함수 적용시켜줌) !!!!!!\n",
    "- 마지막 층이 활성화 함수 거쳤으면 loss에 from_logits=False !!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246f2dae-ed19-4390-9c49-4095cea6dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), # or BinaryCrossentropy\n",
    "    metrics=[\"accuracy\"]  #auc등 존재\n",
    ")\n",
    "\n",
    "# 콜백\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # 검증 손실 모니터링, val_auc도 가능\n",
    "    patience=3,          # 3번 연속 개선 없으면 종료\n",
    "    restore_best_weights=True)\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best_model.h5',  # 저장할 파일 이름\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True)       # 가장 좋은 모델만 저장\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71225772-dbf3-4c19-ae38-f566de075a8f",
   "metadata": {},
   "source": [
    "# 4. tf.data.Dataset으로 fit에 넣을 형태로 만들기\n",
    "- (입력, 라벨)쌍의 튜플 구조\n",
    "- 입력은 여러개도 가능(input_ids, attention_mask, token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c7bd86-b2ba-4ba7-8f0b-b8c5b9a2514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, val, test 분리하기(80%,20%)\n",
    "from sklearn.model_selection import train_test_split # 이걸로 분리\n",
    "\n",
    "#이후 train,val,test에 각각 밑의 코드 실행\n",
    "# Dataset만들기\n",
    "encoded = tokenizer(texts) # train, val, test별로 다 별도 진행\n",
    "labels = tf.convert_to_tensor(labels) # 숫자정답컬럼을 텐서로\n",
    "\n",
    "# dict로 { 'input_ids': ..., 'attention_mask': ..., 'token_type_ids': ... }구조로\n",
    "dataset = tf.data.Dataset.from_tensor_slices((dict(encoded), labels)) # 각 element는 (inputs_dict, label) 튜플 형태\n",
    "\n",
    "# 배치 & 셔플 & 반복 등 처리\n",
    "dataset = dataset.shuffle(100).batch(32).prefetch(tf.data.AUTOTUNE) # dataset은 fit대신 여기서 배치사이즈 지정 !!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b44619-8e8a-4646-bf03-5f45d4ed8a17",
   "metadata": {},
   "source": [
    "# 5. 학습과 저장 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f831db-3e5f-4f70-a9a6-ebecbc3f885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=3,\n",
    "    callbacks=[early_stop, model_checkpoint] \n",
    ")\n",
    "\n",
    "# 저장(hugging face방식): ./saved_model/ 폴더에 모델 가중치(tf_model.h5), config, vocab 등이 저장\n",
    "model.save_pretrained(\"./saved_model\")\n",
    "tokenizer.save_pretrained(\"./saved_model\")\n",
    "\n",
    "# 불러오기\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"./saved_model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./saved_model\")\n",
    "\n",
    "\n",
    "### 커스텀 모델일경우 !!!\n",
    "model.save(\"custom_bert.h5\") # .h5로 저장\n",
    "\n",
    "from your_code import TFBertClassifier  # 정의해놓은 클래스\n",
    "\n",
    "model = TFBertClassifier(num_labels=2)\n",
    "model.load_weights(\"custom_bert.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad2f5db-b41c-4107-9e45-90afd4f9a81b",
   "metadata": {},
   "source": [
    "# 6. 평가와 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb00a94-0715-44f1-bce9-1ef84771c7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가\n",
    "loss, acc = model.evaluate(val_dataset) #손실과 정확도\n",
    "\n",
    "# 예측: 값이 test_dataset로 여러개일땐 model.predict/ 하나일땐 직접 호출\n",
    "def predict_bert(model, tokenizer, texts):\n",
    "    enc = tokenizer(texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "    logits = model(enc).logits\n",
    "    preds = tf.argmax(tf.nn.softmax(logits, axis=-1), axis=-1)\n",
    "    return preds.numpy()\n",
    "\n",
    "predict_bert(model,tokenizer,texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd9beb2-deb1-4c78-b721-e9e2e9bc5ee3",
   "metadata": {},
   "source": [
    "# 7. 중요 텍스트 시각화\n",
    "- 입력 데이터의 어느부분이 중요한지 보여줌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd2d061-1c0d-4baa-8d09-788f67129278",
   "metadata": {},
   "source": [
    "## bertviz로 Attention 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a6453-8970-4eb0-aa4b-d79ec24349dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bertviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ac2db6-677e-4d2d-9984-27cea3b8f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "from bertviz import head_view\n",
    "\n",
    "# 1) 모델과 토크나이저 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 2) 입력 문장 토크나이징\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "inputs = tokenizer(sentence, return_tensors='tf')\n",
    "\n",
    "# 3) 모델에 입력하고 attention weight 얻기\n",
    "outputs = model(inputs, output_attentions=True)\n",
    "attentions = outputs.attentions  # tuple: (layers, batch, heads, seq_len, seq_len)\n",
    "\n",
    "# 4) 토큰 리스트 추출\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].numpy())\n",
    "\n",
    "# 5) bertviz로 시각화\n",
    "head_view(attentions, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fd9572-8906-4669-8a5a-b7b16123248b",
   "metadata": {},
   "source": [
    "## tf.GradientTape()으로 기울기 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7edf62f-5e4e-4cce-bbc2-697a529c938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"This is an example sentence.\"\n",
    "inputs = tokenizer(text, return_tensors=\"tf\")\n",
    "\n",
    "#임베딩 레이어 참고\n",
    "embedding_layer = model.bert.embeddings\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    inputs_embeds = embedding_layer(inputs['input_ids'])\n",
    "    tape.watch(inputs_embeds)\n",
    "    outputs = model(inputs_embeds=inputs_embeds)\n",
    "    loss = outputs.logits[:, 1]\n",
    "\n",
    "grads = tape.gradient(loss, inputs_embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a48f5ae-11b5-46e4-91d1-bef863475cff",
   "metadata": {},
   "source": [
    "## SHAP (SHapley Additive exPlanations)로 각 토큰 별 기여도를 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647164db-8b45-405f-9763-ac8c41e80870",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193e6d8d-0d7f-4e20-afa5-49370113a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import tensorflow as tf\n",
    "\n",
    "# tokenizer, model 준비 완료 상태 가정\n",
    "inputs = tokenizer(..., return_tensors=\"tf\")\n",
    "\n",
    "def f(input_ids_np):\n",
    "    # numpy 배열 → tf.Tensor 변환\n",
    "    inputs_tf = {\"input_ids\": tf.convert_to_tensor(input_ids_np)}\n",
    "    outputs = model(inputs_tf)\n",
    "    probs = tf.nn.softmax(outputs.logits, axis=-1)\n",
    "    return probs.numpy()\n",
    "\n",
    "# 배경 데이터: input_ids만 추출 (배치 크기 10)\n",
    "background = inputs[\"input_ids\"][:10].numpy()\n",
    "\n",
    "explainer = shap.KernelExplainer(f, background)\n",
    "shap_values = explainer.shap_values(inputs[\"input_ids\"].numpy())\n",
    "\n",
    "# 시각화 (샘플 0 기준)\n",
    "shap.summary_plot(shap_values[0], inputs[\"input_ids\"].numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
